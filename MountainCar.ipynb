{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO2B0kZpeoL5BdQwT4KjQaC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KornelWitkowski/Deep-Q-Learning-with-Tensorflow/blob/main/MountainCar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MountainCar"
      ],
      "metadata": {
        "id": "khDAbORTlGbA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l09JYBb6WZcu"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "apt-get install swig\n",
        "\n",
        "git clone https://github.com/pybox2d/pybox2d\n",
        "cd pybox2d\n",
        "python setup.py build\n",
        "python setup.py install\n",
        "\n",
        "apt-get install -y xvfb\n",
        "\n",
        "pip install \\\n",
        "    gym==0.21 \\\n",
        "    gym[box2d]==0.21 \\\n",
        "    pyglet==1.5.27 \\\n",
        "    pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start();"
      ],
      "metadata": {
        "id": "EgZFl9-9WrLC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "class DeepQLearningModel:\n",
        "  def __init__(self, observation_size, hidden_size, action_size, learning_rate=0.001):\n",
        "\n",
        "    self.model = Sequential([Dense(observation_size, activation=\"relu\"),\n",
        "                             Dense(hidden_size, activation=\"relu\"),\n",
        "                             Dense(hidden_size, activation=\"relu\"),\n",
        "                             Dense(action_size-1)])\n",
        "    self.compile()\n",
        "    \n",
        "  def compile(self, learning_rate=0.001):  \n",
        "    self.model.compile(loss=\"mse\",\n",
        "                       optimizer=Adam(learning_rate=learning_rate))\n",
        "    \n",
        "  def fit(self, x, y):\n",
        "    history = self.model.fit(x, y, epochs=1, verbose=0)\n",
        "    loss =  history.history[\"loss\"][0]\n",
        "    return loss"
      ],
      "metadata": {
        "id": "rGqiApB_W-oR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def epsilon_greedy_policy(state, environment, model, epsilon=0.0):\n",
        "\n",
        "  if np.random.random() < epsilon:\n",
        "    action = random.choice([0, 1])\n",
        "  else:\n",
        "    q_values = model(tf.expand_dims(state, axis=0))\n",
        "    action = tf.math.argmax(q_values, axis=1)\n",
        "    action = int(action)\n",
        "  return action"
      ],
      "metadata": {
        "id": "Dmfo4yJtXCXH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque, namedtuple\n",
        "import random\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ],
      "metadata": {
        "id": "XEY2ysACXMrm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ],
      "metadata": {
        "id": "72NjdgUpXUVZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, TimeLimit\n",
        "\n",
        "def create_gym_environment(name):\n",
        "  environment = gym.make(name)\n",
        "  environment = TimeLimit(environment, max_episode_steps=800)\n",
        "  environment = RecordVideo(environment, video_folder='./recored_episodes', episode_trigger=lambda x: x % 50 == 0)\n",
        "  environment = RecordEpisodeStatistics(environment)\n",
        "\n",
        "  return environment"
      ],
      "metadata": {
        "id": "Mutb5WDxXmBX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import pandas as pd\n",
        "\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, TimeLimit\n",
        "\n",
        "import gym\n",
        "\n",
        "class DeepQLearningAlgorithm:\n",
        "\n",
        "    def __init__(self, environment_name=\"MountainCar-v0\", policy=epsilon_greedy_policy, capacity=100_000,\n",
        "                batch_size=256, learning_rate=1e-3, hidden_size=128, gamma=0.99, epsilon_start=1.0, epsilon_end=0.15,\n",
        "                epsilon_last_episode=600, samples_per_epoch=2_048//8, q_net_update_rate=10):\n",
        "    \n",
        "      self.environment = create_gym_environment(environment_name)\n",
        "      observation_size = self.environment.observation_space.shape[0]\n",
        "      actions_size = self.environment.action_space.n\n",
        "\n",
        "      self.q_net = DeepQLearningModel(observation_size, hidden_size, actions_size, learning_rate)\n",
        "      self.target_q_net = copy.deepcopy(self.q_net.model)\n",
        "\n",
        "      self.policy = policy\n",
        "      self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "       \n",
        "      self.current_epoch = 0\n",
        "      self.total_reward = 0\n",
        "\n",
        "\n",
        "      # hyperparameters\n",
        "      self.batch_size = batch_size\n",
        "      self.learning_rate = learning_rate\n",
        "      self.gamma = 0.99\n",
        "      self.epsilon_start = epsilon_start\n",
        "      self.epsilon_end = epsilon_end\n",
        "      self.epsilon_last_episode = epsilon_last_episode\n",
        "      self.samples_per_epoch = samples_per_epoch\n",
        "      self.q_net_update_rate = 10\n",
        "\n",
        "\n",
        "      while len(self.buffer) < self.samples_per_epoch:\n",
        "        self.play_episode(epsilon=0)\n",
        "        \n",
        "    def play_episode(self, policy=None, epsilon=0.):\n",
        "      state = self.environment.reset()\n",
        "      done = False\n",
        "      self.total_reward = 0\n",
        "\n",
        "      while not done:\n",
        "        if policy:\n",
        "          action = policy(state, self.environment, self.q_net.model, epsilon=epsilon)\n",
        "        else:\n",
        "          action = random.choice([0, 1])\n",
        "\n",
        "        next_state, _, done, info = self.environment.step(2*action)\n",
        "\n",
        "        reward = (10 * next_state[1])**2\n",
        "\n",
        "        if next_state[0] >= 0.45:\n",
        "          reward += 100\n",
        "        self.total_reward += reward\n",
        "\n",
        "\n",
        "        exp = (state, action, reward, done, next_state)\n",
        "        self.buffer.append(exp)\n",
        "        state = next_state\n",
        "\n",
        "      return\n",
        "\n",
        "    def get_batch(self):\n",
        "      sample = pd.DataFrame(self.buffer.sample(self.batch_size))\n",
        "\n",
        "      state = np.stack(sample[0].values)\n",
        "      action = np.stack(sample[1].values)\n",
        "      reward = np.stack(sample[2].values)\n",
        "      done = np.stack(sample[3].values)\n",
        "      next_state = np.stack(sample[4].values)\n",
        "\n",
        "      return state, action, reward, done, next_state\n",
        "\n",
        "\n",
        "    def train_step(self):\n",
        "      states, actions, rewards, dones, next_states = self.get_batch()\n",
        "\n",
        "      state_action_values = tf.gather(self.q_net.model(states), actions, axis=1, batch_dims=1)\n",
        "\n",
        "      next_action_values = tf.math.reduce_max(self.target_q_net(next_states), axis=1)\n",
        "      next_action_values = next_action_values.numpy()\n",
        "      next_action_values[dones] = 0.0\n",
        "\n",
        "      expected_state_action_values = rewards + self.gamma * next_action_values\n",
        "\n",
        "      q_net_predictions = self.q_net.model(states).numpy()\n",
        "      q_net_predictions[range(self.batch_size), actions] = expected_state_action_values\n",
        "\n",
        "      loss = self.q_net.fit(states, q_net_predictions)\n",
        "\n",
        "      return loss\n",
        "\n",
        "      \n",
        "    def training_epoch_end(self):\n",
        "\n",
        "        epsilon = max(self.epsilon_end,\n",
        "                      self.epsilon_start - self.current_epoch / self.epsilon_last_episode)\n",
        "\n",
        "        self.play_episode(policy=self.policy, epsilon=epsilon)\n",
        "\n",
        "        if self.current_epoch % self.q_net_update_rate == 0:\n",
        "          self.target_q_net = copy.deepcopy(self.q_net.model)\n",
        "          \n",
        "\n",
        "        return\n",
        "        \n",
        "    def train(self, epochs):\n",
        "\n",
        "      for i in range(epochs):\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        for _ in range(self.samples_per_epoch//self.batch_size):  \n",
        "          loss += self.train_step()\n",
        "\n",
        "        self.training_epoch_end()\n",
        "\n",
        "        if self.current_epoch % 50 == 0:\n",
        "          returns = list(self.environment.return_queue)[-1]\n",
        "          print(f\"Epoch: {self.current_epoch}, loss: {loss}, last total reward: {self.total_reward}\")\n",
        "\n",
        "        self.current_epoch += 1\n"
      ],
      "metadata": {
        "id": "lKNohAHCXy-x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/recored_episodes\n",
        "\n",
        "algorithm =  DeepQLearningAlgorithm(hidden_size=32)"
      ],
      "metadata": {
        "id": "hJJeHlaDYIgx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "algorithm.train(2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_VeU674YOeR",
        "outputId": "2265aa00-3508-4cd8-eb28-824883e040c2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.001230426598340273, last total reward: 0.6439800962735318\n",
            "Epoch: 50, loss: 8.219757000915706e-05, last total reward: 0.8422570181630848\n",
            "Epoch: 100, loss: 0.00016307328769471496, last total reward: 5.858195979504444\n",
            "Epoch: 150, loss: 0.00014067997108213603, last total reward: 7.989061202533485\n",
            "Epoch: 200, loss: 0.0006104748463258147, last total reward: 7.712216994972469\n",
            "Epoch: 250, loss: 0.001088780234567821, last total reward: 6.33884359408185\n",
            "Epoch: 300, loss: 0.009495184756815434, last total reward: 6.720035123371619\n",
            "Epoch: 350, loss: 0.004412907175719738, last total reward: 1.2443257794631468\n",
            "Epoch: 400, loss: 0.02027062140405178, last total reward: 309.93332401415705\n",
            "Epoch: 450, loss: 18.959522247314453, last total reward: 1.199041807604208\n",
            "Epoch: 500, loss: 0.22823959589004517, last total reward: 0.49191510244754344\n",
            "Epoch: 550, loss: 24.51347541809082, last total reward: 309.0028566574047\n",
            "Epoch: 600, loss: 1.0472352504730225, last total reward: 214.9071179456203\n",
            "Epoch: 650, loss: 23.07160758972168, last total reward: 9.319859500025895\n",
            "Epoch: 700, loss: 31.852582931518555, last total reward: 0.9850664154919812\n",
            "Epoch: 750, loss: 32.390621185302734, last total reward: 314.69759086705096\n",
            "Epoch: 800, loss: 70.6162338256836, last total reward: 217.51742750768383\n",
            "Epoch: 850, loss: 57.492027282714844, last total reward: 214.15405280497254\n",
            "Epoch: 900, loss: 156.41017150878906, last total reward: 212.7447200598798\n",
            "Epoch: 950, loss: 65.83023071289062, last total reward: 215.6209773590178\n",
            "Epoch: 1000, loss: 121.78271484375, last total reward: 215.870122572079\n",
            "Epoch: 1050, loss: 194.15200805664062, last total reward: 215.56624233899612\n",
            "Epoch: 1100, loss: 69.25902557373047, last total reward: 211.8839113640272\n",
            "Epoch: 1150, loss: 131.3091583251953, last total reward: 307.03813046213486\n",
            "Epoch: 1200, loss: 25.938838958740234, last total reward: 216.77076621807493\n",
            "Epoch: 1250, loss: 25.205854415893555, last total reward: 312.20147982677315\n",
            "Epoch: 1300, loss: 43.4089469909668, last total reward: 215.84517618198095\n",
            "Epoch: 1350, loss: 144.29791259765625, last total reward: 218.81288048097548\n",
            "Epoch: 1400, loss: 65.2494888305664, last total reward: 314.9735520543318\n",
            "Epoch: 1450, loss: 58.749168395996094, last total reward: 216.8598991478972\n",
            "Epoch: 1500, loss: 78.65100860595703, last total reward: 217.1675581749723\n",
            "Epoch: 1550, loss: 44.77925109863281, last total reward: 217.22310795249\n",
            "Epoch: 1600, loss: 105.73394775390625, last total reward: 216.27232318455418\n",
            "Epoch: 1650, loss: 38.473812103271484, last total reward: 0.7141120226839623\n",
            "Epoch: 1700, loss: 135.5991668701172, last total reward: 217.10688746420666\n",
            "Epoch: 1750, loss: 18.044099807739258, last total reward: 0.20778789805648665\n",
            "Epoch: 1800, loss: 42.798316955566406, last total reward: 218.2786276198251\n",
            "Epoch: 1850, loss: 3.588334083557129, last total reward: 214.590292483244\n",
            "Epoch: 1900, loss: 27.821025848388672, last total reward: 317.0334505605651\n",
            "Epoch: 1950, loss: 30.20690155029297, last total reward: 314.78614730440677\n"
          ]
        }
      ]
    }
  ]
}