{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KornelWitkowski/Deep-Q-Learning-with-Tensorflow/blob/main/LunarLander_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U39ud66f8lLI"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y xvfb\n",
        "\n",
        "!pip install \\\n",
        "    gym==0.21 \\\n",
        "    gym[box2d] \\\n",
        "    pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BMOVj7nKS4Vj"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8CM8cmB1IIV2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "class DeepQLearningModel:\n",
        "  def __init__(self, observation_size, hidden_size, action_size, learning_rate=0.001):\n",
        "\n",
        "    self.model = Sequential([Dense(observation_size, activation=\"relu\"),\n",
        "                             Dense(hidden_size, activation=\"relu\"),\n",
        "                             Dense(hidden_size, activation=\"relu\"),\n",
        "                             Dense(action_size)])\n",
        "    self.compile()\n",
        "    \n",
        "  def compile(self, learning_rate=0.001):  \n",
        "    self.model.compile(loss=\"mse\",\n",
        "                       optimizer=Adam(learning_rate=learning_rate))\n",
        "    \n",
        "  def fit(self, x, y):\n",
        "    history = self.model.fit(x, y, epochs=1, verbose=0)\n",
        "    loss =  history.history[\"loss\"][0]\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gUoPnfamJi_f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def epsilon_greedy_policy(state, environment, model, epsilon=0.0):\n",
        "\n",
        "  if np.random.random() < epsilon:\n",
        "    action = environment.action_space.sample()\n",
        "  else:\n",
        "    q_values = model(tf.expand_dims(state, axis=0))\n",
        "    action = tf.math.argmax(q_values, axis=1)\n",
        "    action = int(action)\n",
        "  return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eBVjpn0F-vi7"
      },
      "outputs": [],
      "source": [
        "from collections import deque, namedtuple\n",
        "import random\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vuKmvd7sJ1nG"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1x-x1tyuKKU-"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, TimeLimit\n",
        "\n",
        "def create_gym_environment(name):\n",
        "  environment = gym.make(name)\n",
        "  environment = TimeLimit(environment, max_episode_steps=400)\n",
        "  environment = RecordVideo(environment, video_folder='./recored_episodes', episode_trigger=lambda x: x % 50 == 0)\n",
        "  environment = RecordEpisodeStatistics(environment)\n",
        "\n",
        "  return environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "55yvuv18f5Kt"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import pandas as pd\n",
        "\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, TimeLimit\n",
        "\n",
        "import gym\n",
        "\n",
        "class DeepQLearningAlgorithm:\n",
        "\n",
        "    def __init__(self, environment_name='LunarLander-v2', policy=epsilon_greedy_policy, capacity=100_000,\n",
        "                batch_size=256, learning_rate=1e-3, hidden_size=128, gamma=0.99, epsilon_start=1.0, epsilon_end=0.15,\n",
        "                epsilon_last_episode=600, samples_per_epoch=2_048, q_net_update_rate=10):\n",
        "    \n",
        "      self.environment = create_gym_environment(environment_name)\n",
        "      observation_size = self.environment.observation_space.shape[0]\n",
        "      actions_size = self.environment.action_space.n\n",
        "\n",
        "      self.q_net = DeepQLearningModel(observation_size, hidden_size, actions_size, learning_rate)\n",
        "      self.target_q_net = copy.deepcopy(self.q_net.model)\n",
        "\n",
        "      self.policy = policy\n",
        "      self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "       \n",
        "      self.current_epoch = 0\n",
        "\n",
        "\n",
        "      # hyperparameters\n",
        "      self.batch_size = batch_size\n",
        "      self.learning_rate = learning_rate\n",
        "      self.gamma = 0.99\n",
        "      self.epsilon_start = epsilon_start\n",
        "      self.epsilon_end = epsilon_end\n",
        "      self.epsilon_last_episode = epsilon_last_episode\n",
        "      self.samples_per_epoch = samples_per_epoch\n",
        "      self.q_net_update_rate = 10\n",
        "\n",
        "\n",
        "      while len(self.buffer) < self.samples_per_epoch:\n",
        "        self.play_episode(epsilon=0)\n",
        "        \n",
        "    def play_episode(self, policy=None, epsilon=0.):\n",
        "      state = self.environment.reset()\n",
        "      done = False\n",
        "\n",
        "      while not done:\n",
        "        if policy:\n",
        "          action = policy(state, self.environment, self.q_net.model, epsilon=epsilon)\n",
        "        else:\n",
        "          action = self.environment.action_space.sample()\n",
        "\n",
        "        next_state, reward, done, info = self.environment.step(action)\n",
        "        exp = (state, action, reward, done, next_state)\n",
        "        self.buffer.append(exp)\n",
        "        state = next_state\n",
        "\n",
        "    def get_batch(self):\n",
        "      sample = pd.DataFrame(self.buffer.sample(self.batch_size))\n",
        "\n",
        "      state = np.stack(sample[0].values)\n",
        "      action = np.stack(sample[1].values)\n",
        "      reward = np.stack(sample[2].values)\n",
        "      done = np.stack(sample[3].values)\n",
        "      next_state = np.stack(sample[4].values)\n",
        "\n",
        "      return state, action, reward, done, next_state\n",
        "\n",
        "\n",
        "    def train_step(self):\n",
        "      states, actions, rewards, dones, next_states = self.get_batch()\n",
        "\n",
        "      state_action_values = tf.gather(self.q_net.model(states), actions, axis=1, batch_dims=1)\n",
        "\n",
        "      next_action_values = tf.math.reduce_max(self.target_q_net(next_states), axis=1)\n",
        "      next_action_values = next_action_values.numpy()\n",
        "      next_action_values[dones] = 0.0\n",
        "\n",
        "      expected_state_action_values = rewards + self.gamma * next_action_values\n",
        "\n",
        "      q_net_predictions = self.q_net.model(states).numpy()\n",
        "      q_net_predictions[range(self.batch_size), actions] = expected_state_action_values\n",
        "\n",
        "      loss = self.q_net.fit(states, q_net_predictions)\n",
        "\n",
        "      return loss\n",
        "\n",
        "      \n",
        "    def training_epoch_end(self):\n",
        "\n",
        "        epsilon = max(self.epsilon_end,\n",
        "                      self.epsilon_start - self.current_epoch / self.epsilon_last_episode)\n",
        "\n",
        "        self.play_episode(policy=self.policy, epsilon=epsilon)\n",
        "\n",
        "        if self.current_epoch % self.q_net_update_rate == 0:\n",
        "          self.target_q_net = copy.deepcopy(self.q_net.model)\n",
        "\n",
        "        return\n",
        "        \n",
        "    def train(self, epochs):\n",
        "\n",
        "      for i in range(epochs):\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        for _ in range(self.samples_per_epoch//self.batch_size):  \n",
        "          loss += self.train_step()\n",
        "\n",
        "        self.training_epoch_end()\n",
        "\n",
        "        if self.current_epoch % 50 == 0:\n",
        "          returns = list(self.environment.return_queue)[-1]\n",
        "          print(f\"Epoch: {self.current_epoch}, loss: {loss}, hp_metric: {tf.math.reduce_mean(returns)}\")\n",
        "\n",
        "        self.current_epoch += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IS8brWKKekB-"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/recored_episodes\n",
        "\n",
        "algorithm =  DeepQLearningAlgorithm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rVmgF249LV2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c83697a-c8f4-4629-a00c-a8efad018250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 268.12224674224854, hp_metric: -181.4905548095703\n",
            "Epoch: 50, loss: 65.59887909889221, hp_metric: -153.10211181640625\n",
            "Epoch: 100, loss: 40.14419388771057, hp_metric: -134.60256958007812\n",
            "Epoch: 150, loss: 36.258490562438965, hp_metric: -120.94712829589844\n",
            "Epoch: 200, loss: 51.76798403263092, hp_metric: -59.935302734375\n",
            "Epoch: 250, loss: 43.43266558647156, hp_metric: -51.29109191894531\n",
            "Epoch: 300, loss: 25.66166865825653, hp_metric: -27.810462951660156\n",
            "Epoch: 350, loss: 48.81535291671753, hp_metric: -23.567724227905273\n",
            "Epoch: 400, loss: 31.40342652797699, hp_metric: -12.756509780883789\n",
            "Epoch: 450, loss: 32.64399337768555, hp_metric: 46.689064025878906\n",
            "Epoch: 500, loss: 28.33835005760193, hp_metric: 53.12044906616211\n",
            "Epoch: 550, loss: 27.020818173885345, hp_metric: 79.65638732910156\n",
            "Epoch: 600, loss: 17.04546356201172, hp_metric: 76.03502655029297\n",
            "Epoch: 650, loss: 18.183525025844574, hp_metric: 77.77593994140625\n",
            "Epoch: 700, loss: 5.392167240381241, hp_metric: 74.37455749511719\n",
            "Epoch: 750, loss: 2.513824075460434, hp_metric: 72.29029083251953\n",
            "Epoch: 800, loss: 2.492232918739319, hp_metric: 86.86651611328125\n",
            "Epoch: 850, loss: 9.343723803758621, hp_metric: 88.17721557617188\n",
            "Epoch: 900, loss: 18.86451354622841, hp_metric: 76.15917205810547\n",
            "Epoch: 950, loss: 18.189046025276184, hp_metric: 50.66957473754883\n",
            "Epoch: 1000, loss: 15.120583534240723, hp_metric: 51.971160888671875\n",
            "Epoch: 1050, loss: 9.79539930820465, hp_metric: 76.8135757446289\n",
            "Epoch: 1100, loss: 13.341503649950027, hp_metric: 90.1175765991211\n",
            "Epoch: 1150, loss: 32.55587261915207, hp_metric: 115.60354614257812\n",
            "Epoch: 1200, loss: 45.98831230401993, hp_metric: 96.49748992919922\n",
            "Epoch: 1250, loss: 13.301397979259491, hp_metric: 132.8748779296875\n",
            "Epoch: 1300, loss: 15.444121330976486, hp_metric: 109.0035171508789\n",
            "Epoch: 1350, loss: 16.03119930624962, hp_metric: 117.92822265625\n",
            "Epoch: 1400, loss: 14.576570749282837, hp_metric: 103.91288757324219\n",
            "Epoch: 1450, loss: 21.82975709438324, hp_metric: 101.72953033447266\n",
            "Epoch: 1500, loss: 14.967108607292175, hp_metric: 100.03087615966797\n",
            "Epoch: 1550, loss: 13.88722974061966, hp_metric: 154.52847290039062\n",
            "Epoch: 1600, loss: 36.94377061724663, hp_metric: 113.21958923339844\n",
            "Epoch: 1650, loss: 29.35711544752121, hp_metric: 133.04298400878906\n",
            "Epoch: 1700, loss: 16.481555342674255, hp_metric: 123.91218566894531\n",
            "Epoch: 1750, loss: 33.20040142536163, hp_metric: 76.85913848876953\n",
            "Epoch: 1800, loss: 45.13676697015762, hp_metric: 111.54325866699219\n",
            "Epoch: 1850, loss: 27.698477864265442, hp_metric: 116.41605377197266\n",
            "Epoch: 1900, loss: 24.783792674541473, hp_metric: 180.1309814453125\n",
            "Epoch: 1950, loss: 23.414385318756104, hp_metric: 147.8792724609375\n"
          ]
        }
      ],
      "source": [
        "algorithm.train(2000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average return for epsilon=0: {tf.math.reduce_mean(list(algorithm.environment.return_queue))}\")"
      ],
      "metadata": {
        "id": "UiH7FvDUYFQg",
        "outputId": "8f1d04b3-6783-4cf4-ee09-877917c373bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average return for epsilon=0: 213.4485321044922\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyNssS20J5Dv5UrRska+7hlE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}