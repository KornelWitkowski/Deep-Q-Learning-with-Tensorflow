{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOV6AJNdMb+cHc1jm5HYDWT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KornelWitkowski/Deep-Q-Learning-with-Tensorflow/blob/main/ContinuousLunarLander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalized advantage function Deep Q-learning"
      ],
      "metadata": {
        "id": "q6m64gVvKntP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9_-wHLHrUzE"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "apt-get install swig\n",
        "\n",
        "git clone https://github.com/pybox2d/pybox2d\n",
        "cd pybox2d\n",
        "python setup.py build\n",
        "python setup.py install\n",
        "\n",
        "apt-get install -y xvfb\n",
        "\n",
        "pip install \\\n",
        "    gym==0.21 \\\n",
        "    gym[box2d]==0.21 \\\n",
        "    pyglet==1.5.27 \\\n",
        "    pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-VDuRY1cp7u",
        "outputId": "fb06a804-4a27-4efd-e8a0-68013be33c4e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f132fc3c100>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Input, Reshape, Add, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "class TrainingModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, model_mu, model_v, model_p):\n",
        "    super().__init__()\n",
        "    self.model_mu = model_mu\n",
        "    self.model_v = model_v\n",
        "    self.model_p = model_p\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x, a = inputs\n",
        "\n",
        "    in_mu = self.model_mu(x)\n",
        "    in_v = self.model_v(x)\n",
        "\n",
        "    mu = Lambda(lambda z: tf.squeeze(z))(in_mu)\n",
        "    v = Lambda(lambda z: tf.squeeze(z))(in_v)\n",
        "    p = self.model_p(x)\n",
        "\n",
        "    P = Lambda(lambda z: z @ tf.transpose(z, perm=[0,2,1]))(p)\n",
        "\n",
        "    u_mu = Lambda(lambda z: tf.expand_dims(a - z, axis=1))(in_mu)\n",
        "    u_mu_t = Lambda(lambda z: tf.transpose(z, perm=[0,2,1]))(u_mu)\n",
        "\n",
        "    adv = - 0.5 * Lambda(lambda z: z[0] @ z[1] @ z[2])([u_mu, P, u_mu_t])\n",
        "\n",
        "    output = Add()([adv, in_v])\n",
        "    output = Lambda(lambda z: tf.squeeze(z))(output)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "class NafDeepQLearningModel:\n",
        "  def __init__(self, observation_size=8, hidden_size=128, action_size=2, learning_rate=0.001):\n",
        "\n",
        "\n",
        "    self.common_layer = Sequential([Input(observation_size),\n",
        "                                   Dense(hidden_size, \"relu\"),\n",
        "                                   Dense(hidden_size, \"relu\")])\n",
        "\n",
        "    self.model_mu = Sequential([self.common_layer,\n",
        "                          Dense(action_size, activation=\"tanh\")])\n",
        "    \n",
        "    self.model_v = Sequential([self.common_layer,\n",
        "                         Dense(1)])\n",
        "\n",
        "    self.model_p = Sequential([self.common_layer,\n",
        "                         Dense(action_size*action_size, activation=\"sigmoid\"),\n",
        "                         Reshape((action_size, action_size))])\n",
        "\n",
        "    self.model = TrainingModel(self.model_mu, self.model_v, self.model_p)\n",
        "\n",
        "    self.compile(learning_rate)\n",
        "\n",
        "  def compile(self, learning_rate=0.001):  \n",
        "    self.model.compile(loss=\"mse\", optimizer=Adam(learning_rate=learning_rate), metrics=[\"mse\"])\n",
        "    \n",
        "  def fit(self, x, y):\n",
        "    history = self.model.fit(x, y, epochs=1, verbose=0)\n",
        "    loss =  history.history[\"loss\"][0]\n",
        "    return loss"
      ],
      "metadata": {
        "id": "P8SFRDZSrfQp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, TimeLimit\n",
        "\n",
        "def create_enviorment(name):\n",
        "  env = gym.make(name)\n",
        "  env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=lambda x: x % 50 == 0)\n",
        "  env = RepeatActionWrapper(env, 4)\n",
        "  env = RecordEpisodeStatistics(env)\n",
        "  return env"
      ],
      "metadata": {
        "id": "v0ooOAtokIcW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "environment = gym.make(\"LunarLanderContinuous-v2\")\n",
        "\n",
        "a_min = environment.action_space.low\n",
        "a_max = environment.action_space.high\n",
        "\n",
        "del environment\n",
        "\n",
        "def noisy_policy(state, env, model, epsilon=0.0):\n",
        "  state = tf.expand_dims(state, axis=0)\n",
        "  mu = model(state)\n",
        "  mu = mu + tf.random.normal(mu.shape, 0, epsilon)\n",
        "  \n",
        "  action = tf.clip_by_value(mu, a_min, a_max)\n",
        "  action = tf.squeeze(action)\n",
        "\n",
        "  return action"
      ],
      "metadata": {
        "id": "I_gwnQdoSEZ8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RepeatActionWrapper(gym.Wrapper):\n",
        "\n",
        "  def __init__(self, env, n):\n",
        "    super().__init__(env)\n",
        "    self.env = env\n",
        "    self.n = n\n",
        "\n",
        "  def step(self, action):\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    for _ in range(self.n):\n",
        "      next_state, reward, done, info = self.env.step(action)\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "    return next_state, total_reward, done, info"
      ],
      "metadata": {
        "id": "uGZu0VKpDOFl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, TimeLimit\n",
        "\n",
        "def create_enviorment(name, n=8):\n",
        "  env = gym.make(name)\n",
        "  env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=lambda x: x % 50 == 0)\n",
        "  env = RepeatActionWrapper(env, n)\n",
        "  env = RecordEpisodeStatistics(env)\n",
        "  return env"
      ],
      "metadata": {
        "id": "ov4QAVID9plK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ],
      "metadata": {
        "id": "JnA8G8n227CA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class DeepQLearningAlgorithm:\n",
        "      def __init__(self, environment_name=\"LunarLanderContinuous-v2\", policy=noisy_policy, capacity=100_000,\n",
        "                batch_size=256, learning_rate=1e-5, hidden_size=512, gamma=0.99, epsilon_start=2.0, epsilon_end=0.02,\n",
        "                epsilon_last_episode=800, samples_per_epoch=1_024, q_net_update_rate=10, n=8):\n",
        "        \n",
        "          self.environment = create_enviorment(environment_name, n)\n",
        "          observation_size = self.environment.observation_space.shape[0]\n",
        "          actions_size = self.environment.action_space.shape[0]\n",
        "\n",
        "          self.q_net = NafDeepQLearningModel(observation_size, hidden_size, actions_size, learning_rate)\n",
        "\n",
        "          self.policy = policy\n",
        "          self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "          self.current_epoch = 0\n",
        "\n",
        "          self.batch_size = batch_size\n",
        "          self.learning_rate = learning_rate\n",
        "          self.gamma = 0.99\n",
        "          self.epsilon_start = epsilon_start\n",
        "          self.epsilon_end = epsilon_end\n",
        "          self.epsilon_last_episode = epsilon_last_episode\n",
        "          self.samples_per_epoch = samples_per_epoch\n",
        "          self.q_net_update_rate = 10\n",
        "\n",
        "          while len(self.buffer) < self.samples_per_epoch:\n",
        "            self.play_episode(epsilon=self.epsilon_start)\n",
        "\n",
        "          self.target_v_model = tf.keras.models.clone_model(self.q_net.model_v)\n",
        "\n",
        "              \n",
        "      def play_episode(self, policy=None, epsilon=0.):\n",
        "          state = self.environment.reset()\n",
        "          done = False\n",
        "\n",
        "          while not done:\n",
        "            if policy:\n",
        "              action = policy(state, self.environment, self.q_net.model_mu, epsilon=epsilon)\n",
        "            else:\n",
        "              action = self.environment.action_space.sample()\n",
        "\n",
        "            next_state, reward, done, info = self.environment.step(action)\n",
        "            exp = (state, action, reward, done, next_state)\n",
        "            self.buffer.append(exp)\n",
        "            state = next_state\n",
        "\n",
        "      def get_batch(self):\n",
        "        \n",
        "        sample = pd.DataFrame(self.buffer.sample(self.batch_size))\n",
        "        \n",
        "        state = np.stack(sample[0].values)\n",
        "        action = np.stack(sample[1].values)\n",
        "        reward = np.stack(sample[2].values)\n",
        "        done = np.stack(sample[3].values)\n",
        "        next_state = np.stack(sample[4].values)\n",
        "\n",
        "        return state, action, reward, done, next_state\n",
        "\n",
        "      def train_step(self):\n",
        "        states, actions, rewards, dones, next_states = self.get_batch()\n",
        "  \n",
        "        next_state_values = self.target_v_model(next_states).numpy()\n",
        "        next_state_values[dones] = 0.\n",
        "        next_state_values = tf.squeeze(next_state_values)\n",
        "\n",
        "        target = rewards + self.gamma * next_state_values\n",
        "\n",
        "        loss = self.q_net.fit((states, actions), target)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "      def training_epoch_end(self):\n",
        "\n",
        "        epsilon = max(self.epsilon_end,\n",
        "                      self.epsilon_start - self.current_epoch / self.epsilon_last_episode)\n",
        "\n",
        "        self.play_episode(policy=self.policy, epsilon=epsilon)\n",
        "\n",
        "        if (self.current_epoch + 1) % self.q_net_update_rate == 0:\n",
        "          self.target_v_model = tf.keras.models.clone_model(self.q_net.model_v)\n",
        "\n",
        "        self.current_epoch += 1          \n",
        "\n",
        "      def train(self, epochs):\n",
        "\n",
        "        for i in range(epochs):\n",
        "\n",
        "          loss = 0\n",
        "\n",
        "          for _ in range(self.samples_per_epoch//self.batch_size):  \n",
        "            loss += self.train_step()/(self.samples_per_epoch//self.batch_size)\n",
        "\n",
        "          self.training_epoch_end()\n",
        "\n",
        "          if self.current_epoch % 50 == 0:\n",
        "            returns = list(self.environment.return_queue)[-1]\n",
        "            print(f\"Epoch: {self.current_epoch}, loss: {loss}, hp_metric: {tf.math.reduce_mean(returns)}\")\n"
      ],
      "metadata": {
        "id": "UQCEulhFgge5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alg = DeepQLearningAlgorithm(n=12, learning_rate=1e-3)\n",
        "alg.train(1000)"
      ],
      "metadata": {
        "id": "g-goFx6--Y5B",
        "outputId": "95d727ca-caa1-4f70-b9fb-a09fa3826f2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 50, loss: 896.4072113037109, hp_metric: -370.2440490722656\n",
            "Epoch: 100, loss: 874.0405426025391, hp_metric: -1256.202392578125\n",
            "Epoch: 150, loss: 795.625732421875, hp_metric: -586.153076171875\n",
            "Epoch: 200, loss: 835.6949462890625, hp_metric: -666.8700561523438\n",
            "Epoch: 250, loss: 811.6567230224609, hp_metric: -675.03076171875\n",
            "Epoch: 300, loss: 772.5878143310547, hp_metric: -651.790771484375\n",
            "Epoch: 350, loss: 708.3697509765625, hp_metric: -550.282470703125\n",
            "Epoch: 400, loss: 638.1910095214844, hp_metric: -215.37681579589844\n",
            "Epoch: 450, loss: 819.2611846923828, hp_metric: -421.616455078125\n",
            "Epoch: 500, loss: 669.8406219482422, hp_metric: -12.224639892578125\n",
            "Epoch: 550, loss: 656.6982421875, hp_metric: -219.802490234375\n",
            "Epoch: 600, loss: 683.8597717285156, hp_metric: -176.8139190673828\n",
            "Epoch: 650, loss: 650.2520599365234, hp_metric: -360.3421325683594\n",
            "Epoch: 700, loss: 622.5941162109375, hp_metric: -138.72171020507812\n",
            "Epoch: 750, loss: 615.2307281494141, hp_metric: -81.72682189941406\n",
            "Epoch: 800, loss: 638.4923095703125, hp_metric: -215.09402465820312\n",
            "Epoch: 850, loss: 574.7903671264648, hp_metric: -347.8689880371094\n",
            "Epoch: 900, loss: 683.5477142333984, hp_metric: -146.30445861816406\n",
            "Epoch: 950, loss: 631.6266174316406, hp_metric: -293.52606201171875\n",
            "Epoch: 1000, loss: 649.5425109863281, hp_metric: -59.090457916259766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alg.train(400)"
      ],
      "metadata": {
        "id": "y23Rg0d0UV4y",
        "outputId": "ef9bcc32-ecd7-4cd9-8344-33ee109bde65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1050, loss: 568.7523193359375, hp_metric: -156.8037567138672\n",
            "Epoch: 1100, loss: 591.5465545654297, hp_metric: -215.9573516845703\n",
            "Epoch: 1150, loss: 538.0778350830078, hp_metric: -124.85720825195312\n",
            "Epoch: 1200, loss: 545.7513580322266, hp_metric: -95.98601531982422\n",
            "Epoch: 1250, loss: 517.0646286010742, hp_metric: -14.822734832763672\n",
            "Epoch: 1300, loss: 551.4833908081055, hp_metric: -184.640625\n",
            "Epoch: 1350, loss: 497.8832092285156, hp_metric: 99.13664245605469\n",
            "Epoch: 1400, loss: 511.9227294921875, hp_metric: -33.31315231323242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probably, in the code is an error or the hyperparameters should be better ajusted. The network is evidently doing better with time. However, it cannot achive very good results."
      ],
      "metadata": {
        "id": "4_iKBjRBQgWG"
      }
    }
  ]
}