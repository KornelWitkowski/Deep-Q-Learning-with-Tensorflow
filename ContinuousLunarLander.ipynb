{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/Kz/3FB22VSNU8jcHIFTU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KornelWitkowski/Deep-Q-Learning-with-Tensorflow/blob/main/ContinuousLunarLander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discretization of continuous action space"
      ],
      "metadata": {
        "id": "jh1ucsw6L0nY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sGK9Cyc5nRj"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "apt-get install swig\n",
        "\n",
        "git clone https://github.com/pybox2d/pybox2d\n",
        "cd pybox2d\n",
        "python setup.py build\n",
        "python setup.py install\n",
        "\n",
        "apt-get install -y xvfb\n",
        "\n",
        "pip install \\\n",
        "    gym==0.21 \\\n",
        "    gym[box2d]==0.21 \\\n",
        "    pyglet==1.5.27 \\\n",
        "    pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "class DeepQLearningModel:\n",
        "  def __init__(self, observation_size, hidden_size, action_size=25, learning_rate=0.001):\n",
        "\n",
        "    self.model = Sequential([Dense(observation_size, activation=\"relu\"),\n",
        "                             Dense(hidden_size, activation=\"relu\"),\n",
        "                             Dense(hidden_size, activation=\"relu\"),\n",
        "                             Dense(action_size)])\n",
        "    self.compile()\n",
        "    \n",
        "  def compile(self, learning_rate=0.001):  \n",
        "    self.model.compile(loss=\"mse\",\n",
        "                       optimizer=Adam(learning_rate=learning_rate))\n",
        "    \n",
        "  def fit(self, x, y):\n",
        "    history = self.model.fit(x, y, epochs=1, verbose=0)\n",
        "    loss =  history.history[\"loss\"][0]\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "2q13HRh4Fgzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actions1 = [-1.0, -0.5, 0.0, 0.5, 1]\n",
        "actions2 = [-1.0, -0.5, 0.0, 0.5, 1]\n",
        "\n",
        "act = []\n",
        "for a1 in actions1:\n",
        "  for a2 in actions2:\n",
        "    act.append([a1, a2])"
      ],
      "metadata": {
        "id": "4OzJkp3QFw_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def epsilon_greedy_policy(state, environment, model, epsilon=0.0):\n",
        "\n",
        "  if np.random.random() < epsilon:\n",
        "    action = random.randrange(len(act))\n",
        "    return action\n",
        "  else:\n",
        "    q_values = model(tf.expand_dims(state, axis=0))\n",
        "    action = tf.math.argmax(q_values, axis=1)\n",
        "    action = int(action)\n",
        "  return action"
      ],
      "metadata": {
        "id": "mWFnhB1nFh6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def epsilon_greedy_policy(state, environment, model, epsilon=0.0):\n",
        "\n",
        "  if np.random.random() < epsilon:\n",
        "    action = random.randrange(len(act))\n",
        "    return action\n",
        "  else:\n",
        "    q_values = model(tf.expand_dims(state, axis=0))\n",
        "    action = tf.math.argmax(q_values, axis=1)\n",
        "    action = int(action)\n",
        "  return action"
      ],
      "metadata": {
        "id": "u7aj_2SuLV1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ],
      "metadata": {
        "id": "bT64iWE2GI14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, TimeLimit\n",
        "\n",
        "def create_environment(name, n=8):\n",
        "  env = gym.make(name)\n",
        "  env = RecordVideo(env, video_folder='./recored_episodes', episode_trigger=lambda x: x % 50 == 0)\n",
        "  env = RepeatActionWrapper(env, n)\n",
        "  env = RecordEpisodeStatistics(env)\n",
        "  return env"
      ],
      "metadata": {
        "id": "BrM81ggyLbRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ],
      "metadata": {
        "id": "69b9S1LYiyN5",
        "outputId": "ab5f647a-e46a-4d0f-e3e6-80e76b054779",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7ff593a645e0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import pandas as pd\n",
        "\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, TimeLimit\n",
        "\n",
        "import gym\n",
        "\n",
        "class DeepQLearningAlgorithm:\n",
        "\n",
        "    def __init__(self, environment_name=\"LunarLanderContinuous-v2\", policy=epsilon_greedy_policy, capacity=100_000,\n",
        "                batch_size=256, learning_rate=1e-3, hidden_size=128, gamma=0.99, epsilon_start=1.0, epsilon_end=0.15,\n",
        "                epsilon_last_episode=600, samples_per_epoch=2_048//8, q_net_update_rate=10, repeat_action=8):\n",
        "\n",
        "      self.environment = create_environment(environment_name, repeat_action)\n",
        "      observation_size = self.environment.observation_space.shape[0]\n",
        "      actions_size = len(act)\n",
        "\n",
        "      self.q_net = DeepQLearningModel(observation_size, hidden_size, actions_size, learning_rate)\n",
        "      self.target_q_net = copy.deepcopy(self.q_net.model)\n",
        "\n",
        "      self.policy = policy\n",
        "      self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "       \n",
        "      self.current_epoch = 0\n",
        "\n",
        "      # hyperparameters\n",
        "      self.batch_size = batch_size\n",
        "      self.learning_rate = learning_rate\n",
        "      self.gamma = 0.99\n",
        "      self.epsilon_start = epsilon_start\n",
        "      self.epsilon_end = epsilon_end\n",
        "      self.epsilon_last_episode = epsilon_last_episode\n",
        "      self.samples_per_epoch = samples_per_epoch\n",
        "      self.q_net_update_rate = 10\n",
        "\n",
        "      while len(self.buffer) < self.samples_per_epoch:\n",
        "        self.play_episode(epsilon=0)\n",
        "\n",
        "    def play_episode(self, policy=None, epsilon=0.):\n",
        "      state = self.environment.reset()\n",
        "      done = False\n",
        "\n",
        "      while not done:\n",
        "        if policy:\n",
        "          action = policy(state, self.environment, self.q_net.model, epsilon=epsilon)\n",
        "        else:\n",
        "          action = epsilon_greedy_policy(state, self.environment, self.q_net.model, epsilon=1.0)\n",
        "\n",
        "        next_state, reward, done, info = self.environment.step(act[action])\n",
        "        exp = (state, action, reward, done, next_state)\n",
        "        self.buffer.append(exp)\n",
        "        state = next_state\n",
        "\n",
        "    def get_batch(self):\n",
        "      sample = pd.DataFrame(self.buffer.sample(self.batch_size))\n",
        "\n",
        "      state = np.stack(sample[0].values)\n",
        "      action = np.stack(sample[1].values)\n",
        "      reward = np.stack(sample[2].values)\n",
        "      done = np.stack(sample[3].values)\n",
        "      next_state = np.stack(sample[4].values)\n",
        "\n",
        "      return state, action, reward, done, next_state\n",
        "\n",
        "\n",
        "    def train_step(self):\n",
        "      states, actions, rewards, dones, next_states = self.get_batch()\n",
        "\n",
        "      state_action_values = tf.gather(self.q_net.model(states), actions, axis=1, batch_dims=1)\n",
        "\n",
        "      next_action_values = tf.math.reduce_max(self.target_q_net(next_states), axis=1)\n",
        "      next_action_values = next_action_values.numpy()\n",
        "      next_action_values[dones] = 0.0\n",
        "\n",
        "\n",
        "      expected_state_action_values = rewards + self.gamma * next_action_values\n",
        "\n",
        "      q_net_predictions = self.q_net.model(states).numpy()\n",
        "      q_net_predictions[range(self.batch_size), actions] = expected_state_action_values\n",
        "\n",
        "      loss = self.q_net.fit(states, q_net_predictions)\n",
        "\n",
        "      return loss\n",
        "\n",
        "      \n",
        "    def training_epoch_end(self):\n",
        "\n",
        "        epsilon = max(self.epsilon_end,\n",
        "                      self.epsilon_start - self.current_epoch / self.epsilon_last_episode)\n",
        "\n",
        "        self.play_episode(policy=self.policy, epsilon=epsilon)\n",
        "\n",
        "        if self.current_epoch % self.q_net_update_rate == 0:\n",
        "          self.target_q_net = copy.deepcopy(self.q_net.model)\n",
        "\n",
        "        return\n",
        "\n",
        "        \n",
        "    def train(self, epochs):\n",
        "\n",
        "      for i in range(epochs):\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        for _ in range(self.samples_per_epoch//self.batch_size):  \n",
        "          loss += self.train_step()\n",
        "        self.training_epoch_end()\n",
        "\n",
        "        if self.current_epoch % 50 == 0:\n",
        "          returns = list(self.environment.return_queue)[-1]\n",
        "          print(f\"Epoch: {self.current_epoch}, loss: {loss}, hp_metric: {tf.math.reduce_mean(returns)}\")\n",
        "\n",
        "        self.current_epoch += 1\n"
      ],
      "metadata": {
        "id": "cRE7j2x6GOh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/recored_episodes\n",
        "\n",
        "alg =  DeepQLearningAlgorithm(\n",
        "                              hidden_size=256, gamma=0.96, epsilon_start=1.0, epsilon_end=0.05,\n",
        "                              epsilon_last_episode=1000, samples_per_epoch=2_048//2, repeat_action=4)\n",
        "\n",
        "alg.train(1000)"
      ],
      "metadata": {
        "id": "vPGdGHzDu5t7",
        "outputId": "6d61f1bd-de81-402d-cd59-92a608b1f741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 102.16668319702148, hp_metric: -562.28662109375\n",
            "Epoch: 50, loss: 21.141337394714355, hp_metric: -76.96858978271484\n",
            "Epoch: 100, loss: 16.298704862594604, hp_metric: 35.937355041503906\n",
            "Epoch: 150, loss: 23.156526803970337, hp_metric: -322.248291015625\n",
            "Epoch: 200, loss: 16.39613103866577, hp_metric: -95.52153778076172\n",
            "Epoch: 250, loss: 16.127256870269775, hp_metric: -136.18785095214844\n",
            "Epoch: 300, loss: 22.193091869354248, hp_metric: -10.861907958984375\n",
            "Epoch: 350, loss: 19.891846656799316, hp_metric: -30.140968322753906\n",
            "Epoch: 400, loss: 19.16852378845215, hp_metric: -8.832000732421875\n",
            "Epoch: 450, loss: 30.884401321411133, hp_metric: -34.16162872314453\n",
            "Epoch: 500, loss: 25.083611965179443, hp_metric: -142.4987335205078\n",
            "Epoch: 550, loss: 23.650888442993164, hp_metric: -40.66557693481445\n",
            "Epoch: 600, loss: 20.624852180480957, hp_metric: 11.072304725646973\n",
            "Epoch: 650, loss: 22.892570734024048, hp_metric: 62.71717071533203\n",
            "Epoch: 700, loss: 26.163174152374268, hp_metric: 0.17111873626708984\n",
            "Epoch: 750, loss: 24.373328685760498, hp_metric: 239.42686462402344\n",
            "Epoch: 800, loss: 27.550204753875732, hp_metric: 188.8101348876953\n",
            "Epoch: 850, loss: 17.635048866271973, hp_metric: 285.04425048828125\n",
            "Epoch: 900, loss: 18.25031566619873, hp_metric: 257.284423828125\n",
            "Epoch: 950, loss: 38.909247398376465, hp_metric: 264.272216796875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalized advantage function Deep Q-learning"
      ],
      "metadata": {
        "id": "q6m64gVvKntP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9_-wHLHrUzE"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "apt-get install swig\n",
        "\n",
        "git clone https://github.com/pybox2d/pybox2d\n",
        "cd pybox2d\n",
        "python setup.py build\n",
        "python setup.py install\n",
        "\n",
        "apt-get install -y xvfb\n",
        "\n",
        "pip install \\\n",
        "    gym==0.21 \\\n",
        "    gym[box2d]==0.21 \\\n",
        "    pyglet==1.5.27 \\\n",
        "    pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-VDuRY1cp7u",
        "outputId": "fb06a804-4a27-4efd-e8a0-68013be33c4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f132fc3c100>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Input, Reshape, Add, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "class TrainingModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, model_mu, model_v, model_p):\n",
        "    super().__init__()\n",
        "    self.model_mu = model_mu\n",
        "    self.model_v = model_v\n",
        "    self.model_p = model_p\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x, a = inputs\n",
        "\n",
        "    in_mu = self.model_mu(x)\n",
        "    in_v = self.model_v(x)\n",
        "\n",
        "    mu = Lambda(lambda z: tf.squeeze(z))(in_mu)\n",
        "    v = Lambda(lambda z: tf.squeeze(z))(in_v)\n",
        "    p = self.model_p(x)\n",
        "\n",
        "    P = Lambda(lambda z: z @ tf.transpose(z, perm=[0,2,1]))(p)\n",
        "\n",
        "    u_mu = Lambda(lambda z: tf.expand_dims(a - z, axis=1))(in_mu)\n",
        "    u_mu_t = Lambda(lambda z: tf.transpose(z, perm=[0,2,1]))(u_mu)\n",
        "\n",
        "    adv = - 0.5 * Lambda(lambda z: z[0] @ z[1] @ z[2])([u_mu, P, u_mu_t])\n",
        "\n",
        "    output = Add()([adv, in_v])\n",
        "    output = Lambda(lambda z: tf.squeeze(z))(output)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "class NafDeepQLearningModel:\n",
        "  def __init__(self, observation_size=8, hidden_size=128, action_size=2, learning_rate=0.001):\n",
        "\n",
        "\n",
        "    self.common_layer = Sequential([Input(observation_size),\n",
        "                                   Dense(hidden_size, \"relu\"),\n",
        "                                   Dense(hidden_size, \"relu\")])\n",
        "\n",
        "    self.model_mu = Sequential([self.common_layer,\n",
        "                          Dense(action_size, activation=\"tanh\")])\n",
        "    \n",
        "    self.model_v = Sequential([self.common_layer,\n",
        "                         Dense(1)])\n",
        "\n",
        "    self.model_p = Sequential([self.common_layer,\n",
        "                         Dense(action_size*action_size, activation=\"sigmoid\"),\n",
        "                         Reshape((action_size, action_size))])\n",
        "\n",
        "    self.model = TrainingModel(self.model_mu, self.model_v, self.model_p)\n",
        "\n",
        "    self.compile(learning_rate)\n",
        "\n",
        "  def compile(self, learning_rate=0.001):  \n",
        "    self.model.compile(loss=\"mse\", optimizer=Adam(learning_rate=learning_rate), metrics=[\"mse\"])\n",
        "    \n",
        "  def fit(self, x, y):\n",
        "    history = self.model.fit(x, y, epochs=1, verbose=0)\n",
        "    loss =  history.history[\"loss\"][0]\n",
        "    return loss"
      ],
      "metadata": {
        "id": "P8SFRDZSrfQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, TimeLimit\n",
        "\n",
        "def create_enviorment(name):\n",
        "  env = gym.make(name)\n",
        "  env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=lambda x: x % 50 == 0)\n",
        "  env = RepeatActionWrapper(env, 4)\n",
        "  env = RecordEpisodeStatistics(env)\n",
        "  return env"
      ],
      "metadata": {
        "id": "v0ooOAtokIcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "environment = gym.make(\"LunarLanderContinuous-v2\")\n",
        "\n",
        "a_min = environment.action_space.low\n",
        "a_max = environment.action_space.high\n",
        "\n",
        "del environment\n",
        "\n",
        "def noisy_policy(state, env, model, epsilon=0.0):\n",
        "  state = tf.expand_dims(state, axis=0)\n",
        "  mu = model(state)\n",
        "  mu = mu + tf.random.normal(mu.shape, 0, epsilon)\n",
        "  \n",
        "  action = tf.clip_by_value(mu, a_min, a_max)\n",
        "  action = tf.squeeze(action)\n",
        "\n",
        "  return action"
      ],
      "metadata": {
        "id": "I_gwnQdoSEZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RepeatActionWrapper(gym.Wrapper):\n",
        "\n",
        "  def __init__(self, env, n):\n",
        "    super().__init__(env)\n",
        "    self.env = env\n",
        "    self.n = n\n",
        "\n",
        "  def step(self, action):\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    for _ in range(self.n):\n",
        "      next_state, reward, done, info = self.env.step(action)\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "    return next_state, total_reward, done, info"
      ],
      "metadata": {
        "id": "uGZu0VKpDOFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, TimeLimit\n",
        "\n",
        "def create_enviorment(name, n=8):\n",
        "  env = gym.make(name)\n",
        "  env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=lambda x: x % 50 == 0)\n",
        "  env = RepeatActionWrapper(env, n)\n",
        "  env = RecordEpisodeStatistics(env)\n",
        "  return env"
      ],
      "metadata": {
        "id": "ov4QAVID9plK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ],
      "metadata": {
        "id": "JnA8G8n227CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class DeepQLearningAlgorithm:\n",
        "      def __init__(self, environment_name=\"LunarLanderContinuous-v2\", policy=noisy_policy, capacity=100_000,\n",
        "                batch_size=256, learning_rate=1e-5, hidden_size=512, gamma=0.99, epsilon_start=2.0, epsilon_end=0.02,\n",
        "                epsilon_last_episode=800, samples_per_epoch=1_024, q_net_update_rate=10, n=8):\n",
        "        \n",
        "          self.environment = create_enviorment(environment_name, n)\n",
        "          observation_size = self.environment.observation_space.shape[0]\n",
        "          actions_size = self.environment.action_space.shape[0]\n",
        "\n",
        "          self.q_net = NafDeepQLearningModel(observation_size, hidden_size, actions_size, learning_rate)\n",
        "\n",
        "          self.policy = policy\n",
        "          self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "          self.current_epoch = 0\n",
        "\n",
        "          self.batch_size = batch_size\n",
        "          self.learning_rate = learning_rate\n",
        "          self.gamma = 0.99\n",
        "          self.epsilon_start = epsilon_start\n",
        "          self.epsilon_end = epsilon_end\n",
        "          self.epsilon_last_episode = epsilon_last_episode\n",
        "          self.samples_per_epoch = samples_per_epoch\n",
        "          self.q_net_update_rate = 10\n",
        "\n",
        "          while len(self.buffer) < self.samples_per_epoch:\n",
        "            self.play_episode(epsilon=self.epsilon_start)\n",
        "\n",
        "          self.target_v_model = tf.keras.models.clone_model(self.q_net.model_v)\n",
        "\n",
        "              \n",
        "      def play_episode(self, policy=None, epsilon=0.):\n",
        "          state = self.environment.reset()\n",
        "          done = False\n",
        "\n",
        "          while not done:\n",
        "            if policy:\n",
        "              action = policy(state, self.environment, self.q_net.model_mu, epsilon=epsilon)\n",
        "            else:\n",
        "              action = self.environment.action_space.sample()\n",
        "\n",
        "            next_state, reward, done, info = self.environment.step(action)\n",
        "            exp = (state, action, reward, done, next_state)\n",
        "            self.buffer.append(exp)\n",
        "            state = next_state\n",
        "\n",
        "      def get_batch(self):\n",
        "        \n",
        "        sample = pd.DataFrame(self.buffer.sample(self.batch_size))\n",
        "        \n",
        "        state = np.stack(sample[0].values)\n",
        "        action = np.stack(sample[1].values)\n",
        "        reward = np.stack(sample[2].values)\n",
        "        done = np.stack(sample[3].values)\n",
        "        next_state = np.stack(sample[4].values)\n",
        "\n",
        "        return state, action, reward, done, next_state\n",
        "\n",
        "      def train_step(self):\n",
        "        states, actions, rewards, dones, next_states = self.get_batch()\n",
        "  \n",
        "        next_state_values = self.target_v_model(next_states).numpy()\n",
        "        next_state_values[dones] = 0.\n",
        "        next_state_values = tf.squeeze(next_state_values)\n",
        "\n",
        "        target = rewards + self.gamma * next_state_values\n",
        "\n",
        "        loss = self.q_net.fit((states, actions), target)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "      def training_epoch_end(self):\n",
        "\n",
        "        epsilon = max(self.epsilon_end,\n",
        "                      self.epsilon_start - self.current_epoch / self.epsilon_last_episode)\n",
        "\n",
        "        self.play_episode(policy=self.policy, epsilon=epsilon)\n",
        "\n",
        "        if (self.current_epoch + 1) % self.q_net_update_rate == 0:\n",
        "          self.target_v_model = tf.keras.models.clone_model(self.q_net.model_v)\n",
        "\n",
        "        self.current_epoch += 1          \n",
        "\n",
        "      def train(self, epochs):\n",
        "\n",
        "        for i in range(epochs):\n",
        "\n",
        "          loss = 0\n",
        "\n",
        "          for _ in range(self.samples_per_epoch//self.batch_size):  \n",
        "            loss += self.train_step()/(self.samples_per_epoch//self.batch_size)\n",
        "\n",
        "          self.training_epoch_end()\n",
        "\n",
        "          if self.current_epoch % 50 == 0:\n",
        "            returns = list(self.environment.return_queue)[-1]\n",
        "            print(f\"Epoch: {self.current_epoch}, loss: {loss}, hp_metric: {tf.math.reduce_mean(returns)}\")\n"
      ],
      "metadata": {
        "id": "UQCEulhFgge5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alg = DeepQLearningAlgorithm(n=12, learning_rate=1e-3)\n",
        "alg.train(1000)"
      ],
      "metadata": {
        "id": "g-goFx6--Y5B",
        "outputId": "95d727ca-caa1-4f70-b9fb-a09fa3826f2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 50, loss: 896.4072113037109, hp_metric: -370.2440490722656\n",
            "Epoch: 100, loss: 874.0405426025391, hp_metric: -1256.202392578125\n",
            "Epoch: 150, loss: 795.625732421875, hp_metric: -586.153076171875\n",
            "Epoch: 200, loss: 835.6949462890625, hp_metric: -666.8700561523438\n",
            "Epoch: 250, loss: 811.6567230224609, hp_metric: -675.03076171875\n",
            "Epoch: 300, loss: 772.5878143310547, hp_metric: -651.790771484375\n",
            "Epoch: 350, loss: 708.3697509765625, hp_metric: -550.282470703125\n",
            "Epoch: 400, loss: 638.1910095214844, hp_metric: -215.37681579589844\n",
            "Epoch: 450, loss: 819.2611846923828, hp_metric: -421.616455078125\n",
            "Epoch: 500, loss: 669.8406219482422, hp_metric: -12.224639892578125\n",
            "Epoch: 550, loss: 656.6982421875, hp_metric: -219.802490234375\n",
            "Epoch: 600, loss: 683.8597717285156, hp_metric: -176.8139190673828\n",
            "Epoch: 650, loss: 650.2520599365234, hp_metric: -360.3421325683594\n",
            "Epoch: 700, loss: 622.5941162109375, hp_metric: -138.72171020507812\n",
            "Epoch: 750, loss: 615.2307281494141, hp_metric: -81.72682189941406\n",
            "Epoch: 800, loss: 638.4923095703125, hp_metric: -215.09402465820312\n",
            "Epoch: 850, loss: 574.7903671264648, hp_metric: -347.8689880371094\n",
            "Epoch: 900, loss: 683.5477142333984, hp_metric: -146.30445861816406\n",
            "Epoch: 950, loss: 631.6266174316406, hp_metric: -293.52606201171875\n",
            "Epoch: 1000, loss: 649.5425109863281, hp_metric: -59.090457916259766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alg.train(400)"
      ],
      "metadata": {
        "id": "y23Rg0d0UV4y",
        "outputId": "ef9bcc32-ecd7-4cd9-8344-33ee109bde65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1050, loss: 568.7523193359375, hp_metric: -156.8037567138672\n",
            "Epoch: 1100, loss: 591.5465545654297, hp_metric: -215.9573516845703\n",
            "Epoch: 1150, loss: 538.0778350830078, hp_metric: -124.85720825195312\n",
            "Epoch: 1200, loss: 545.7513580322266, hp_metric: -95.98601531982422\n",
            "Epoch: 1250, loss: 517.0646286010742, hp_metric: -14.822734832763672\n",
            "Epoch: 1300, loss: 551.4833908081055, hp_metric: -184.640625\n",
            "Epoch: 1350, loss: 497.8832092285156, hp_metric: 99.13664245605469\n",
            "Epoch: 1400, loss: 511.9227294921875, hp_metric: -33.31315231323242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probably, in the code is an error or the hyperparameters should be better ajusted. The network is evidently doing better with time. However, it cannot achive very good results."
      ],
      "metadata": {
        "id": "4_iKBjRBQgWG"
      }
    }
  ]
}